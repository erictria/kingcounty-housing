---
title: "Project 2"
author: "Eve Schoenrock"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Section 2
### Data Visualizations

```{r}
library(tidyverse)
library(dplyr)
library(MASS)
library(ggplot2)
library(leaps)
library(gridExtra)
library(ROCR)
library(maps)
library(ggthemes)

Data <-read.csv("kc_house_data.csv", header=TRUE)
```

```{r}
Data[,'condition']<-factor(Data[,'condition'])
class(Data$condition)
```

```{r}
##price by condition
ggplot(Data, aes(x=condition, y=log(price)))+
  geom_boxplot(aes(fill=condition)) + 
  labs(x="Condition", y="Price (log of)", color="Price Bracket",
      title="Distribution of Homes in King County by Price Bracket") +
  theme_economist() + 
  scale_fill_economist()+
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, vjust=5))+
  theme(plot.title = element_text(vjust = 1))
```

```{r}
#Change zipcode to a factor
Data$zipcode <- as.factor(Data$zipcode)
```

```{r}
price_brackets <- quantile(Data$price, c(0, .25, .50, .75, 1))
Data$price_cat <- cut(Data$price, price_brackets, 
                    labels=c("Low", "Lower Middle", "Upper Middle", "High"),
                    include.lowest=TRUE) 
```

```{r}
# outline of King County WA
washington <- map_data("county", region="washington")
kingcounty <- subset(washington, subregion=="king")
```

```{r}
##price map
ggplot() +
  geom_path(data=kingcounty, aes(x=long, y=lat, group=group), color="black") +
  geom_point(data=Data, aes(x = long, y = lat, alpha = 0.1, color=price_cat)) +
  labs(x="Longitude", y="Latitude", color="Price Bracket",
      title="Distribution of Homes in King County by Price Bracket") +
  coord_quickmap() +
  scale_alpha(guide = 'none') +
  theme_economist() + 
  scale_color_economist()+
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5, vjust=5))+
  theme(plot.title = element_text(vjust = 1))
```

```{r}
library(GGally)
library(lubridate)
```

```{r}
data <- read.csv('kc_house_data.csv')

#set the random number generator so same results can
#be reproduced
set.seed(1)

#choose the observations to be in the training set.
#splitting the data set 80-20
sample <- sample.int(nrow(data), floor(.50*nrow(data)), replace = F)
train <- data[sample, ] #training data
test <- data[-sample, ] #test data
```

```{r}
colnames(train)
```

```{r}
train <- subset(train, select = -c(id, date, long, lat))
```

```{r}
# linear regression visualizations
train <- subset(train, bedrooms != '33') #drop outlier
```

```{r}
zipcodes <- data.frame(zipcode = c(98004, 98005, 98052, 98121, 98007, 98109, 98033, 98122,
                         98029, 98006, 98103, 98102, 98074, 98101, 98040, 98115,
                         98112, 98107, 98119, 98105, 98075, 98008, 98116, 98053,
                         98034, 98039, 98144, 98199, 98117, 98104, 98028, 98027,
                         98011, 98177, 98125, 98065, 98072, 98077, 98126, 98155,
                         98136, 98059, 98133, 98118, 98106),
                       area_bin = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                         1, 1, 1, 1, 1, 1, 1))
expanded_kc <- merge(x = train, y = zipcodes, by = 'zipcode', all.x = T)

filtered_kc <- expanded_kc %>%
  mutate(
    area_bin = factor(ifelse(!is.na(area_bin), 1, 0)), ## using the grades from Niche
    renovated = factor(ifelse(yr_renovated > 0, 1, 0)),
    waterfront = factor(waterfront),
  ) %>%
  filter(bedrooms < 33) ## outlier likely entered incorrectly
```

```{r}
ggplot(filtered_kc, aes(x=area_bin, y=price)) + geom_boxplot(fill='orange') + labs(x='Area',y='Price($)', title='Variation in Prices by Type of Area')
```

```{r}
par(oma=c(0,0,2,0))
ggplot(train, aes(x=grade)) + geom_histogram(fill='blue') + labs(title='Types and Frequencies of House Grades') + theme(plot.title=element_text(hjust=0.5, vjust=1))
```

```{r}
#checking relationships between the quantitative variables part 1
# 1. "date" 2. "price" 3. "bedrooms" 4. "bathrooms" 5. "sqft_living" 
# 6. "sqft_lot" 7. "floors" 8. "waterfront" 9. "view" 10. "condition"   
# 11. "grade" 12. "yr_built" 13. "yr_renovated" 14. "lat" 15. "long"     

quant_plot_rooms <- ggpairs(data=train, columns=c(2:4),
    axisLabels="show")
quant_plot_rooms
```

```{r}
#checking relationships between the quantitative variables part 2
# 1. "date" 2. "price" 3. "bedrooms" 4. "bathrooms" 5. "sqft_living" 
# 6. "sqft_lot" 7. "floors" 8. "waterfront" 9. "view" 10. "condition"   
# 11. "grade" 12. "yr_built" 13. "yr_renovated" 14. "lat" 15. "long"     

quant_plot_area <- ggpairs(data=train, columns=c(2, 5:7),
    axisLabels="show")
quant_plot_area
```

#### Univariate Visualizations
```{r}
ggplot(train, aes(x=bedrooms)) + geom_bar(fill='orange') + labs(title='Frequency of Number of Bedrooms')
```

```{r}
ggplot(train, aes(x=bathrooms)) + geom_bar(fill='brown') + labs(title='Frequency of Number of Bathrooms')
```

```{r}
ggplot(train, aes(x=log(price))) + geom_histogram(fill='dark green', bins=50) + labs(title='Frequency of log(Price) of House Sales') + theme(plot.title=element_text(hjust=0.5, vjust=1))
#hist(log(data$price))
```

#### Bivariate Visualizations
```{r}
# bivariate visualizations
options(scipen=999)

ggplot(train, aes(x=sqft_living, y=price)) + geom_point(color='purple') + labs(x='Living Space (sqft)', y='Price (S)', title='Price vs Square-Feet Living Space') + theme(plot.title=element_text(hjust=0.5, vjust=1))
```

```{r}
ggplot(train, aes(x=factor(bedrooms), y=log(price))) + geom_boxplot(fill='red') + geom_smooth(method=lm) + labs(x='Number of Bedrooms', title = 'Variation in Prices by Number of Bedrooms')
```

```{r}
ggplot(train, aes(x=factor(bathrooms), y=log(price))) + geom_boxplot(fill='pink') + geom_smooth(method=lm) + labs(x='Number of Bathrooms', title = 'Variation in Prices by Number of Bathrooms')
```

```{r}
ggplot(train, aes(x=factor(waterfront), y=price)) + geom_boxplot(color='blue') + geom_smooth(method=lm) + labs(title = 'Variation in Prices by Waterfront')
```

```{r}
quantile(data$price)
```

```{r}
ggplot(train, aes(x=sqft_living, y=price, color=sqft_lot)) + geom_point(color='purple') + labs(title='Price vs Sqft_Living and Sqft_Lot')
```

#### Multivariate Visualizations

```{r}
# bivariate visualizations
options(scipen=999)
ggplot(train, aes(x=sqft_living, y=price, color=sqft_lot)) + geom_point() + labs(title='Price vs Square-Feet Living Space and Square-Feet Lot Space')
```

```{r}
ggplot(train, aes(x=yr_built, color=factor(waterfront))) + geom_density() + labs(x='Year Built', title='Density of House Built by Year and Waterfront Feature of Home') + theme(plot.title=element_text(hjust=0.5, vjust=1))
```

## Section 3

```{r}
library(leaps)
library(MASS)
library(tidyverse)
library(faraway)
library(lawstat)
```

```{r}
kc <- read.csv('kc_house_data.csv', header = TRUE)
```

```{r}
# zipcodes with overall grades of atleast an A on https://www.niche.com/places-to-live/search/best-zip-codes-to-live/c/king-county-wa/?publicSchoolsGrade=a

zipcodes <- data.frame(zipcode = c(98004, 98005, 98052, 98121, 98007, 98109, 98033, 98122,
                         98029, 98006, 98103, 98102, 98074, 98101, 98040, 98115,
                         98112, 98107, 98119, 98105, 98075, 98008, 98116, 98053,
                         98034, 98039, 98144, 98199, 98117, 98104, 98028, 98027,
                         98011, 98177, 98125, 98065, 98072, 98077, 98126, 98155,
                         98136, 98059, 98133, 98118, 98106),
                       area_bin = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                         1, 1, 1, 1, 1, 1, 1))
expanded_kc <- merge(x = kc, y = zipcodes, by = 'zipcode', all.x = T)
```

```{r}
filtered_kc <- expanded_kc %>%
  mutate(
    area_bin = factor(ifelse(!is.na(area_bin), 1, 0)), ## using the grades from Niche
    renovated = factor(ifelse(yr_renovated > 0, 1, 0)),
    waterfront = factor(waterfront),
  ) %>%
  filter(bedrooms < 33) ## outlier likely entered incorrectly

set.seed(1) ## for reproducibility to get the same split
sample<-sample.int(nrow(filtered_kc), floor(.50*nrow(filtered_kc)), replace = F)
train<-filtered_kc[sample, ] ## training data frame
test<-filtered_kc[-sample, ] # #test data frame
```

```{r}
sqft_cols <- c('sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15')
count_cols <- c('bedrooms', 'bathrooms', 'floors')
index_cols <- c('view', 'condition', 'grade')
```

```{r}
train[, c('price', sqft_cols)] %>%
  cor
```

- From the features measured in square feet, sqft_living has the highest correlation with price at 0.7077887.
- sqft_living is highly correlated with sqft_above and sqft_living15.
- Based on the description, sqft_living should be the sum of sqft_above and sqft_basement.
- From the features measured in square feet, we will use sqft_living as a predictor for the model.

```{r}
train[, c('price', count_cols)] %>%
  cor
```

- Since the number of bathrooms and bedrooms are usually used for housing sales, we will use those two factors as predictors.

```{r}
train[, c('price', index_cols)] %>%
  cor
```

- From the index features, grade has the highest correlation with price at 0.6697105.
- We will use grade as a predictor in the model.
- We will also use view since our model factors in the location of a house.

In addition to these, we will use location features as predictors:
- area_bin
- waterfront

We will also be using the age of the house and if it has been renovated or not:
- yr_built
- renovated

```{r}
train <- train %>%
  subset(select = c(price, sqft_living, bedrooms, bathrooms, grade, area_bin, waterfront, view, yr_built, renovated)) ##training data frame

test <- test %>%
  subset(select = c(price, sqft_living, bedrooms, bathrooms, grade, area_bin, waterfront, view, yr_built, renovated)) ##test data frame
```

```{r}
train %>%
  subset(select = c(price, sqft_living, bedrooms, bathrooms, grade, view, yr_built)) %>%
  pairs(lower.panel = NULL)
```

```{r}
regnull <- lm(price ~ 1, data = train) ##intercept-only model
regfull <- lm(price ~ ., data = train) ##model with all the predictors
```

#### Stepwise
```{r}
step(regnull, scope=list(lower = regnull, upper = regfull), direction = 'both')
```
#### Regression Equation after Stepwise Regression:
(Starting with the intercept-only model)

Predictors selected: All the initial predictors were selected.

```{r}
step(regfull, scope=list(lower = regnull, upper = regfull), direction = 'both')
```
#### Regression Equation after Stepwise Regression:
(Starting with the model with all predictors)

Predictors selected: No predictors were dropped. Similar with the previous stepwise regression, all predictors were selected.

```{r}
step(regnull, scope=list(lower = regnull, upper = regfull), direction = 'forward')
```
#### Regression Equation after Forward Selection:

Predictors selected: All the initial predictors were selected.

```{r}
step(regfull, scope = list(lower = regnull, upper = regfull), direction = 'backward')
```
#### Regression Equation after Backward Elimination:

Predictors selected: No predictors were eliminated. Same with forward selection, all the initial predictors were selected.

```{r}
summary(regfull)
```
The predictors in the full regression model all had significant individual t-tests. 

Although all the t-tests are significant, we noticed that `bedrooms` and `yr_built` have negative coefficients, which is the opposite of their individual correlations with price. We will conduct a reduced F test to check if we can drop those predictors.

```{r}
reduced <- lm(price ~ sqft_living + bathrooms + grade + area_bin + waterfront + view + renovated, data = train)
anova(reduced, regfull)
```

With a p-value of 2.2e-16, which is smaller than 0.05, we reject the null hypothesis that `bedrooms` and `yr_built` are insignificant. We will proceed with using the full model.

We will proceed to validate the full model.

#### Detecting multicollinearity

```{r}
library(faraway)
vif(regfull)
```

Based on the VIFs, there are no signs of multicollinearity in the model.

#### Regression Assumptions
```{r}
res <- data.frame(fitted = regfull$fitted.values, residuals = regfull$residuals)
res %>%
  ggplot(aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red') +
  labs(x = 'Fitted y', y = 'Residuals', title = 'Residual Plot')
```

Based on the residual plot, we can see signs of curvature and heteroskedasticity, which means assumptions 1 (linearity) and 2 (constant variance) are both not met yet. We will proceed with transforming the response variable (price) in order to meet assumption 2. If assumption 1 is still not met after transforming price, we will consider transforming a predictor variable.

```{r}
boxcox(regfull, lambda = seq(0, 0.4, 0.01))
```
To determine what transformation to apply to the price, we generated a Box-Cox plot and found that the log-likelihood maximizing value of lambda seemed to lie between 0.01 and 0.05. To maintain interpretability of the model coefficients, we will select $\lambda = 0$, since it is close to the log-likelihood maximizing range. We will be applying a log transformation to price.

Transformation applied:

$y^* = log(y)$

```{r}
train <- train %>% 
  mutate(log_price = log(price))

transformed <- lm(log_price ~ sqft_living + bedrooms + bathrooms + grade + 
    area_bin + waterfront + view + yr_built + renovated, data = train)
summary(transformed)
```

```{r}
res <- data.frame(fitted = transformed$fitted.values, residuals = transformed$residuals)
res %>%
  ggplot(aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red') +
  labs(x = 'Fitted y', y = 'Residuals', title = 'Residual Plot')
```

Based on the residual plot of the model after transforming y, we can see that the variance is much better spread out now for assumption 2. There is still signs of curvature in the plot so we will consider transforming a predictor variable. 

We will perform a log transformation on `sqft_living` since it is one of the predictors that is highly right skewed.
```{r}
train %>%
  ggplot(aes(x=sqft_living))+
  geom_histogram()+
  labs(x = 'Square Feet of Living', title="Histogram for Square Feet of Living")
```

```{r}
train <- train %>% 
  mutate(log_sqft_living = log(sqft_living))

transformed_x_y <- lm(log_price ~ log_sqft_living + bedrooms + bathrooms + grade + view + yr_built + 
    area_bin + waterfront + renovated, data = train)
summary(transformed_x_y)
```
```{r}
res <- data.frame(fitted = transformed_x_y$fitted.values, residuals = transformed_x_y$residuals)
res %>%
  ggplot(aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red') +
  labs(x = 'Fitted y', y = 'Residuals', title = 'Residual Plot')
```
Based on the residual plot, we can see that the curvature problem has been resolved. Both assumptions 1 and 2 are met.

```{r}
par(mar = c(5,4,0,2) + 0.1)
acf(transformed_x_y$residuals, adj = 0.5)
title('ACF Plot of Residuals', line = -1)
```
Based on the ACF plot of the residuals, the error terms appear to be uncorrelated. Insignificant ACFs at lag 1 and greater indicate that the residuals are uncorrelated, so assumption 3, uncorrelated errors, is met.

```{r}
qqnorm(transformed_x_y$residuals)
qqline(transformed_x_y$residuals, col = 'red')
```
The Normal Q-Q plot shows that, although our error terms do appear to be normally distributed for most of the theoretical quantiles, this relationship tends to break down at the extremes of the graph. Since this is the least necessary condition for linear regression to be valid, especially when the other assumptions are met, we decided to move forward with using the linear regression model.

#### Levene's test
```{r}
ggplot(train, aes(x = waterfront, y = log_price))+
  geom_boxplot()+
  labs(x = 'Waterfront', y = 'log(Price)', title = 'Dist of log(Price) by Waterfront Category')
```
```{r}
library(lawstat)
levene.test(train$log_price, train$waterfront)
```

```{r}
levene.test(train$log_price, train$renovated)
```

```{r}
levene.test(train$log_price, train$area_bin)
```
For each categorical predictor used in this model, we conducted a Levene's test. However, all of the results failed to support the assumption that variances are equal across all classes. Based on the nature of the data, we expect this to happen. In the dataset, houses that are ar the waterfront or have been renovated are in the minority. 
```{r}
table(train$waterfront)
```
```{r}
table(train$renovated)
```

For `area_bin`, the split used for this analysis is separating the really good areas from the rest. Because of that, the result of the Levene's test is expected.

Check ratio of largest variance with lowest variance:
```{r}
renovated_var_0 <- var(train$log_price[train$renovated == 0])
renovated_var_1 <- var(train$log_price[train$renovated == 1])

max(renovated_var_0, renovated_var_1) < (1.5 * min(renovated_var_0, renovated_var_1))
max(renovated_var_0, renovated_var_1) / min(renovated_var_0, renovated_var_1)
```

```{r}
waterfront_var_0 <- var(train$log_price[train$waterfront == 0])
waterfront_var_1 <- var(train$log_price[train$waterfront == 1])

max(waterfront_var_0, waterfront_var_1) < (1.5 * min(waterfront_var_0, waterfront_var_1))
max(waterfront_var_0, waterfront_var_1) / min(waterfront_var_0, waterfront_var_1)
```


```{r}
area_bin_var_0 <- var(train$log_price[train$area_bin == 0])
area_bin_var_1 <- var(train$log_price[train$area_bin == 1])

max(area_bin_var_0, area_bin_var_1) < (1.5 * min(area_bin_var_0, area_bin_var_1))
max(area_bin_var_0, area_bin_var_1) / min(area_bin_var_0, area_bin_var_1)
```
Ratio of largest and lowest variances:

$renovated: 1.48$

$waterfront: 1.59$

$area\ bin: 1.39$

Since ratio off the variances for `renovated` and `area_bin` are below 1.5 and `waterfront` is close at 1.59, we will proceed with validating our model after the additional check of variance ratios.

#### Validation

```{r}
compute_press <- function(model) { 
  residuals <- summary(model)$residuals 
  hat_diagonals <- influence(model)$hat
  pr <- residuals/(1 - hat_diagonals)
  press <- sum(pr^2)
  return(press) 
}

press <- compute_press(transformed_x_y)
press

# use anova() to get the sum of squares for the linear model
lm_anova <- anova(transformed_x_y)
sst <- sum(lm_anova$'Sum Sq')

# calculate the predictive R^2
pred_r_squared <- 1 - press/(sst)
pred_r_squared
```

$PRESS = 649.6636$

$R^2 = 0.7839$

$R^2_a = 0.7837$

$R^2_{pred} = 0.7834337$

Based on $R^2_{pred}, we can say that 78.34% of the variation in the response variable on new observations can be explained by our model.

Since all $R^2$ values are close to each other, we do not have a strong indication of overfitting.

```{r}
# test MSE
test <- test %>%
  mutate(
    log_price = log(price),
    log_sqft_living = log(sqft_living)
  )

test_mse <- mean((test$log_price - predict.lm(transformed_x_y, test)) ^ 2)
test_mse
```
$Test\ MSE = 0.0599879$

```{r}
test_mse_values <- data.frame(actual = test$log_price, fitted = predict.lm(transformed_x_y, test))
test_mse_values %>%
  ggplot(aes(x = actual, y = fitted)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(x = 'Actual log_price', y = 'Fitted log_price', title = 'Fitted y Against Actual y for the Test Data')
```

The above graph shows the linear relationship between the fitted and actual values in the test data set.

#### Residual Analysis

```{r}
ext.student.res <- rstudent(transformed_x_y)
student.res <- rstandard(transformed_x_y)
```
```{r}
res.frame <- data.frame(student.res, ext.student.res)
```

```{r}
n<-dim(train)[1]
p<-9
crit<-qt(1-0.05/(2*n), n-p-1)
```

```{r}
ext.student.res[abs(ext.student.res) > crit]
```
We have 5 outliers at observations 1022, 10903, 10998, 10999, and 19124

```{r}
##outlier observation
train[c('1022', '10903', '10998', '10999', '19124'),]
```

#### Leverage Points

```{r}
lev <- lm.influence(transformed_x_y)$hat ##identify high leverage points
length(lev[lev>2*p/n])
```

We have a lot of leverage points. The next step is to check if these leverage points are influential.

Since the goal of our model is predict overall prices, we will check for influential points based on Cook's distance.
```{r}
##cooks distance
COOKS <- cooks.distance(transformed_x_y)
COOKS[COOKS>qf(0.5,p,n-p)]
```

We have no influential points based on Cook's distance.

#### Partial Regression Plots
```{r}
generate_partial_regression_plot <- function(response, predictor, predictors, lm_data) {
  filtered_predictors <- predictors[! predictors %in% c(predictor)]
  predictors_str <- paste(filtered_predictors, sep = ' + ')
  lm_no_x_str <- paste(response, predictors_str, sep = ' ~ ')
  x_lm_str <- paste(predictor, predictors_str, sep = ' ~ ')
  lm_no_x <- lm(lm_no_x_str, data = lm_data)
  x_lm <- lm(x_lm_str, data = lm_data)
  
  partial_data_x <- data.frame(x_res = summary(x_lm)$residuals, y_res = summary(lm_no_x)$residuals)
  
  x_label <- paste(predictor, 'Residuals', collapse = ' ')
  y_label <- paste('log(Price) Residuals excluding', predictor, collapse = ' ')
  title_str <- paste('Partial Regression Plot for', predictor, collapse = ' ')
  
  partial_data_x %>%
    ggplot(aes(x = x_res, y = y_res)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE) +
    labs(x = x_label, y = y_label, title = title_str)
}
```

```{r}
predictors = c('log_sqft_living', 'bedrooms', 'bathrooms', 'grade', 'view', 'yr_built', 'area_bin', 'waterfront', 'renovated')
```
```{r}
generate_partial_regression_plot('log_price', 'log_sqft_living', predictors, train)
```
```{r}
generate_partial_regression_plot('log_price', 'bedrooms', predictors, train)
```

```{r}
generate_partial_regression_plot('log_price', 'bathrooms', predictors, train)
```
```{r}
generate_partial_regression_plot('log_price', 'grade', predictors, train)
```
```{r}
generate_partial_regression_plot('log_price', 'view', predictors, train)
```

```{r}
generate_partial_regression_plot('log_price', 'yr_built', predictors, train)
```
Partial regression plots of the continuous predictors show linear patterns. This confirms that they can be added as linear predictors in the model.

#### Interpreting Coefficients
```{r}
summary(transformed_x_y)
```
```{r}
intercept_factor <- exp(14.4226479)
intercept_factor
  
# for every 1% increase in sqft_living, we expect 0.4549205% increase in response
# can use taylor series since coefficient is smaller in magnitude
sqft_living_factor <- 0.4549205
sqft_living_factor

bedrooms_factor <- exp(-0.0280571)
bedrooms_factor

bathrooms_factor <- exp(0.0606392)
bathrooms_factor

grade_factor <- exp(0.1591307)
grade_factor

view_factor <- exp(0.0650174)
view_factor

yr_built_factor <- exp(-0.0032341)
yr_built_factor

area_bin_factor <- exp(0.4468567)
area_bin_factor

waterfront_factor <- exp(0.4633652)
waterfront_factor

renovated_factor <- exp(0.0416109)
renovated_factor
```

## Section 4

```{r, include=FALSE}
library(tidyverse)
library(leaps) ##stepwise
```

```{r}
home <- read.delim('kc_house_data.csv', sep=',')
```

```{r}
zipcodes <- data.frame('zip'=c(98004, 98005, 98052, 98121, 98007, 98109, 98033, 98122,
                         98029, 98006, 98103, 98102, 98074, 98101, 98040, 98115,
                         98112, 98107, 98119, 98105, 98075, 98008, 98116, 98053,
                         98034, 98039, 98144, 98199, 98117, 98104, 98028, 98027, 
                         98011, 98177, 98125, 98065, 98072, 98077, 98126, 98155,
                         98136, 98059, 98133, 98118, 98106),
                       'A'=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                         1, 1, 1, 1, 1, 1, 1))
```


```{r}
head(home)
```

```{r}
homie <- home %>%
  mutate(grade_bin = ifelse(grade > 7, 1, 0), ##1=excellent, 0=poor/average
         renovated = ifelse(yr_renovated > 0, 1, 0))
head(homie)
```

```{r}
homie <- homie %>%
  subset(select = c(grade_bin,
         price,
         bedrooms,
         bathrooms,
         sqft_living,
         sqft_lot,
         floors,
         waterfront,
         view,
         yr_built,
         renovated)) %>%
  filter(bedrooms < 33) ##outlier likely entered incorrectly
```

```{r}
set.seed(1) ##for reproducibility to get the same split
sample<-sample.int(nrow(homie), floor(.50*nrow(homie)), replace = F)
train<-homie[sample, ] ##training data frame
test<-homie[-sample, ] ##test data frame
```

```{r, warning=FALSE}
regnull3 <- glm(grade_bin~1, family='binomial', data=train)
regfull3 <- glm(grade_bin~., family='binomial', data=train)
```

#### Stepwise regressions to choose variables for question

Forward
```{r, warning=FALSE}
step(regnull3, scope=list(lower=regnull3, upper=regfull3), direction = 'forward')
```

Backward
```{r, warning=FALSE}
step(regfull3, scope=list(lower=regnull3, upper=regfull3), direction="backward")
```

Stepwise
```{r, warning=FALSE}
step(regnull3, scope=list(lower=regnull3, upper=regfull3), direction="both")
```

Use all predictors according to forward, backward, and stepwise regression analysis.

```{r}
reg <- glm(grade_bin~.-sqft_lot, data=train)
summary(reg)
```

Drop `sqft_lot` because insignificant p-value.

#### Checking multicollinearity

```{r}
cor(train[,-c(1,6)])
```

It looks like `sqft_living` is highly correlated (>.7) with `price` and `bathrooms`.

```{r}
library(faraway)
vif(reg)
```

However, no VIFs are greater than 10 so we will continue assuming that there is no multicollinearity present.

### Detecting Outliers, high leverage, and influential observations

```{r}
ext.student.res<-rstudent(reg)
```

```{r}
n<-dim(train)[1]
p<-10
crit<-qt(1-0.05/(2*n), n-p-1)
```

```{r}
ext.student.res[abs(ext.student.res)>crit]
```

We 3 outliers, according to the external studentized residuals, at observations 3915, 4412, and 1449.

```{r}
##observation 1449
train[c('1449'),]
```


```{r}
##observation 4412
train[c('4412'),]
```

```{r}
##observation 3915
train[c('3915'),]
```

Let's check leverages

```{r}
lev<-lm.influence(reg)$hat ##identify high leverage points
lengths(list(lev[lev>2*p/n]))
```

We have a lot of leverage points. This is probably because housing markets differ across Seattle and we are not controlling for neighborhood.

Influential points based on Cook's distance
```{r}
##cooks distance
COOKS<-cooks.distance(reg)
COOKS[COOKS>qf(0.5,p,n-p)]
```

We have no influential points. We will proceed with the data and regression as is.

### Logistic Regression 1

```{r}
prop.table(table(train$grade_bin))
```

The training data is split with 52.3% of homes being low or average quality and 47.7% of homes being of excellent quality (based on construction).

```{r}
prop.table(table(test$grade_bin))
```

Essentially, the same split is found in the test data (52.2%/47.8%).

```{r}
##change to factor for visuals
train$grade_bin<-factor(train$grade_bin)
levels(train$grade_bin) <- c("Average/Low Quality","Excellent Quality")

train$waterfront<-factor(train$waterfront)
levels(train$waterfront) <- c("No","Yes")

train$view<-factor(train$view)
levels(train$view) <- c("None", "Fair","Average", "Good", "Excellent")

train$renovated<-factor(train$renovated)
levels(train$renovated)<-c('No', 'Yes')
```

```{r}
##waterfront
ggplot(train, aes(x=waterfront, fill=grade_bin)) +
  geom_bar(position = "fill") +
  labs(x="Water-Front Property", y="Proportion",
       title="Home Quality by Water-Front Status")
```

We see that most waterfront properties, over 75% of them, are of excellent quality in construction. This is somewhat counter-intuitive to the coefficient on `waterfront` found in the logistic regression, but when controlling for all other predictors things could have changed.

```{r}
##view
ggplot(train, aes(x=view, fill=grade_bin)) +
  geom_bar(position = "fill") +
  labs(x="View of Property", y="Proportion",
       title="Home Quality by View of Property") +
  theme(plot.title=element_text(hjust=0.5, vjust=5))
```

Properties where the view of the property is good or great tend to be constructed with excellent quality (over 75%). Likewise, when there is no view of the property homes tend to be constructed with average/low quality.

```{r}
##price
ggplot(train, aes(x=grade_bin, y=log(price)))+
  geom_boxplot()+
  labs(title="Distribution of Home Price by Construction Quality",
       x = 'Construction Quality', y = 'log(Price)') +
  theme(plot.title=element_text(hjust=0.5, vjust=1))
```

```{r}
##bedrooms
ggplot(train, aes(x=bedrooms, color=grade_bin))+
  geom_density()+
  labs(title="Dist of Bedrooms by Construction Quality")
```

Most excellently constructed homes have more bedrooms, whereas most average/low quality homes have fewer bedrooms. The divide in quality appears to happen around 3 bedrooms. Again, this is counter-intuitive from the coefficient from the logistic regression model, but when controlling for all other predictors there could have been a change in relationship.

```{r}
##bathrooms
ggplot(train, aes(x=grade_bin, y=bathrooms))+
  geom_boxplot()+
  labs(title="Dist of Bathrooms by Construction Quality")
```

It looks like bathrooms are more indicative of construction quality. Those constructed with excellent quality have a median number of bathrooms greater than 2 while those of lesser quality have a median number of bathrooms less than 2. Excellent quality homes have a much smaller bathroom range than lower quality homes.

```{r}
##sqft_living
ggplot(train, aes(x=grade_bin, y=log(sqft_living)))+
  geom_boxplot()+
  labs(title="Distribution of Square Feet of Living Space by Construction Quality",
       y='log(Square Feet of Living Space)', x='Construction Quality') +
  theme(plot.title=element_text(hjust=0.5, vjust=1))
```

Excellently constructed homes are often larger than poorly/averagely constructed homes.

```{r}
##floors
ggplot(train, aes(x=grade_bin, y=floors))+
  geom_boxplot()+
  labs(title="Dist of Floors by Construction Quality")
```

```{r}
##yr_built
ggplot(train, aes(x=yr_built, color=grade_bin))+
  geom_density()+
  labs(title="Distribution of Construction Dates by Construction Quality",
       y='Density', x='Year Built') +
  theme(plot.title=element_text(hjust=0.5, vjust=1))
```

Better quality homes were built more recently.

```{r}
##renovated
ggplot(train, aes(x=renovated, fill=grade_bin)) +
  geom_bar(position = "fill") +
  labs(x="Renovation Status", y="Proportion",
       title="Home Quality by Renovation Status")
```

There appears to be no difference in renovation status by quality of construction. However, when controlling for other predictors a more meaningful relationship could develop here.

### Hypothesis tests
We will call the regression obtained from stepwise analysis the full model. My hunch is that we could get by with fewer predictors. The predictors I think are valuable based on EDA are: `yr_built`, `floors`, `sqft_living`, `bathrooms`, `price`, `view`, and `waterfront`. A model with these predictors will be considered the reduced model.

```{r, warning=FALSE}
full <- reg
reduced <- glm(grade_bin~yr_built+floors+sqft_living+bathrooms+price+view+waterfront, family='binomial', data = train)
```

```{r}
##test if additional predictors have coefficients equal to 0 ##test stat
TS2<-reduced$deviance-full$deviance
##pvalue
1-pchisq(TS2,9-7)
```

Based on the p-value, we reject the null hypothesis and prefer the larger model.

### Evaluating Predictive Ability

```{r}
library(ROCR)
```

```{r}
##predicted survival rate for test data based on training data
preds<-predict(reg,newdata=test, type="response")
##transform the input data into a format that is suited for the ##performance() function
rates<-prediction(preds, test$grade_bin)
##store the true positive and false positive rates
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
```

**ROC Curve**
```{r}
plot(roc_result, main="ROC Curve for King County Housing")
lines(x = c(0,1), y = c(0,1), col="red")
```

Our logistic regression does better than random guessing.

**AUC**
```{r}
auc<-performance(rates, measure = "auc")
auc@y.values
```

This also confirms our logistic regression does better than random guessing. An AUC of 0.90 is incredibly promising, since 1 is the best possible value.

**Confusion Matrix**
```{r}
table(test$grade_bin, preds>0.5)
```

```{r}
error_rate <- (905+1254)/(4737+905+1254+3910)
print(error_rate)
accuracy <- 1-error_rate
print(accuracy)
```

```{r}
fpr <- 905/(4737+905)
print(fpr)
fnr <- 1254/(1254+3910)
print(fnr)
```

We are comfortable with the cutoff of 0.5 because the overall accuracy is 80% and we have only a 16% false positive rate and 24% false negative rate. This means that we will only predict a home to be of higher quality than it is 16% of the time - this is of greatest importance because we are most concerned with accurately predicting whether or not a home was constructed with excellent quality.

```{r}
table(test$grade_bin, preds>0.4)
```

```{r}
error_rate <- (757+1295)/(4347+757+1295+4407)
print(error_rate)
accuracy <- 1-error_rate
print(accuracy)
```

```{r}
fpr <- 1295/(4347+1295)
print(fpr)
fnr <- 757/(757+4407)
print(fnr)
```

Here is better accuracy (81%) but a worse false positive rate (23%).

```{r}
table(test$grade_bin, preds>0.42)
```

```{r}
error_rate <- (502+1614)/(4028+1614+502+4662)
print(error_rate)
accuracy <- 1-error_rate
print(accuracy)
```

A cutoff of 0.42 does not change the accuracy by much, and the false positive rate increases, so we prefer the 0.50 cutoff. 