---
title: "Project 2 - MLR"
author: "Group 7"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(leaps)
library(MASS)
library(tidyverse)
library(faraway)
```

```{r}
kc <- read.csv('kc_house_data.csv', header = TRUE)
```

```{r}
filtered_kc <- kc %>%
  mutate(
    renovated = factor(ifelse(yr_renovated > 0, 1, 0)),
    view_bin = factor(view), # changed to factor since price and view has no clear linear relationship
    waterfront = factor(waterfront),
  ) %>%
  subset(select = -c(id, date, zipcode, yr_renovated))

set.seed(1) ##for reproducibility to get the same split
sample<-sample.int(nrow(filtered_kc), floor(.50*nrow(filtered_kc)), replace = F)
train<-filtered_kc[sample, ] ##training data frame
test<-filtered_kc[-sample, ] ##test data frame
```

```{r}
sqft_cols <- c('sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15')
count_cols <- c('bedrooms', 'bathrooms', 'floors')
index_cols <- c('view', 'condition', 'grade')
binary_col <- c('waterfront', 'grade_bin')
```
```{r}
train[, c('price', sqft_cols)] %>%
  cor
```

- From the features measured in square feet, sqft_living has the highest correlation with price at 0.7077887.
- sqft_living is highly correlated with sqft_above and sqft_living15.
- Based on the description, sqft_living should be the sum of sqft_above and sqft_basement.
- From the features measured in square feet, we will use sqft_living as a predictor for the model.

```{r}
train[, c('price', count_cols)] %>%
  cor
```

- Since the number of bathrooms and bedrooms are usually used for housing sales, we will use those two factors as predictors.

```{r}
train[, c('price', index_cols)] %>%
  cor
```

- From the index features, grade has the highest correlation with price at 0.6697105.
- We will use grade as a predictor in the model

In addition to these, we will use location features as predictors:
- long
- lat
- waterfront
- view_bin (factorized version of view)

We will also be using recency of the building:
- yr_built
- renovated


```{r}
filtered_kc <- filtered_kc %>%
  subset(select = c(price, sqft_living, bedrooms, bathrooms, grade, lat, long, waterfront, view, yr_built, renovated))

set.seed(1) ##for reproducibility to get the same split
sample<-sample.int(nrow(filtered_kc), floor(.50*nrow(filtered_kc)), replace = F)
train<-filtered_kc[sample, ] ##training data frame
test<-filtered_kc[-sample, ] ##test data frame
```

```{r}
regnull <- lm(price ~ 1, data = train)
regfull <- lm(price ~ ., data = train)
```

### Stepwise
```{r}
step(regnull, scope=list(lower = regnull, upper = regfull), direction = 'both')
```
```{r}
step(regfull, scope=list(lower = regnull, upper = regfull), direction = 'both')
```
```{r}
summary(regfull)
```
```{r}
reduced <- lm(price ~ sqft_living + bedrooms + bathrooms + grade + view + lat + long + yr_built + waterfront, data = train)
anova(reduced, regfull)
```
Based on ANOVA F test, we reject the null and go with the reduced model without `renovated`

```{r}
summary(reduced)
```
### Detecting multicollinearity

```{r}
vif(reduced)
```

Based on the VIFs, there are no signs of multicollinearity.

### Check for interaction terms??

### Regression Assumptions
```{r}
res <- data.frame(reduced$fitted.values, reduced$residuals)
res %>%
  ggplot(aes(x = reduced.fitted.values, y = reduced.residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red') +
  labs(x = 'Fitted y', y = 'Residuals', title = 'Residual Plot')
```
```{r}
 acf(reduced$residuals, main = 'ACF Plot of Residuals')
```
```{r}
qqnorm(reduced$residuals)
qqline(reduced$residuals, col = 'red')
```
```{r}
boxcox(reduced, lambda = seq(0, 0.4, 0.01))
```
Transform the response variable
y* = log(y)
Applied log so that the coefficients can still be interpreted.

```{r}
train <- train %>% 
  mutate(ystar = log(price))

transformed <- lm(ystar ~ sqft_living + bedrooms + bathrooms + grade + view + lat + long + yr_built + waterfront, data = train)
summary(transformed)
```
`long` appears to be insignificant after transforming `price`

```{r}
transformed_reduced <- lm(ystar ~ sqft_living + bedrooms + bathrooms + grade + view + lat + yr_built + waterfront, data = train)
summary(transformed_reduced)
```

```{r}
res <- data.frame(fitted = transformed_reduced$fitted.values, residuals = transformed_reduced$residuals)
res %>%
  ggplot(aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red') +
  labs(x = 'Fitted y', y = 'Residuals', title = 'Residual Plot')
```
```{r}
acf(transformed_reduced$residuals, main = 'ACF Plot of Residuals')
```
```{r}
qqnorm(transformed_reduced$residuals)
qqline(transformed_reduced$residuals, col = 'red')
```
```{r}
boxcox(transformed_reduced)
```
Passes the regression assumptions after transforming y.
Will check with prof woo/xiang tomorrow if box cox is okay (near 1)

### Validation

```{r}
compute_press <- function(model) { 
  residuals <- summary(model)$residuals 
  hat_diagonals <- influence(model)$hat
  pr <- residuals/(1 - hat_diagonals)
  press <- sum(pr^2)
  return(press) 
}

press <- compute_press(transformed_reduced)
press

# use anova() to get the sum of squares for the linear model
lm_anova <- anova(transformed_reduced)
sst <- sum(lm_anova$'Sum Sq')

# calculate the predictive R^2
pred_r_squared <- 1 - press/(sst)
pred_r_squared
```

$R^2 = 0.7585$

$R^2_a = 0.7583$

$R^2_{pred} = 0.758044$

75.80% of the variation in the response
variable on new observations can be explained by our model.

Since all $R^2$ values are close to each other, not indicative of overfitting.


### Residual Analysis

```{r}
ext.student.res <- rstudent(transformed_reduced)
student.res <- rstandard(transformed_reduced)
```
```{r}
res.frame <- data.frame(student.res, ext.student.res)
```

```{r}
n<-dim(train)[1]
p<-8
crit<-qt(1-0.05/(2*n), n-p-1)
```

```{r}
ext.student.res[abs(ext.student.res)>crit]
```
We have 3 outliers at observations 2590, 4025, and 12552

```{r}
##observation 1449
train[c('2590', '4025', '12552'),]
```

### Leverage Points

```{r}
lev<-lm.influence(transformed_reduced)$hat ##identify high leverage points
#lev[lev>2*p/n]
```

We have a lot of leverage points.

Influential points based on Cook's distance
```{r}
##cooks distance
COOKS<-cooks.distance(transformed_reduced)
COOKS[COOKS>qf(0.5,p,n-p)]
```

We have no influential points. We will proceed with the data and regression as is.

### Partial Regression Plots
```{r}
generate_partial_regression_plot <- function(response, predictor, predictors, lm_data) {
  filtered_predictors <- predictors[! predictors %in% c(predictor)]
  predictors_str <- paste(filtered_predictors, sep = ' + ')
  lm_no_x_str <- paste(response, predictors_str, sep = ' ~ ')
  x_lm_str <- paste(predictor, predictors_str, sep = ' ~ ')
  lm_no_x <- lm(lm_no_x_str, data = lm_data)
  x_lm <- lm(x_lm_str, data = lm_data)
  
  partial_data_x <- data.frame(x_res = summary(x_lm)$residuals, y_res = summary(lm_no_x)$residuals)
  
  x_label <- paste(predictor, 'Residuals', collapse = ' ')
  y_label <- paste('price Residuals excluding', predictor, collapse = ' ')
  title_str <- paste('Partial Regression Plot for', predictor, collapse = ' ')
  
  partial_data_x %>%
    ggplot(aes(x = x_res, y = y_res)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE) +
    labs(x = x_label, y = y_label, title = title_str)
}
```

```{r}
predictors = c('sqft_living', 'bedrooms', 'bathrooms', 'grade', 'view', 'lat', 'yr_built', 'waterfront')
generate_partial_regression_plot('ystar', 'sqft_living', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'bedrooms', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'bathrooms', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'grade', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'view', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'lat', predictors, train)
```
```{r}
generate_partial_regression_plot('ystar', 'yr_built', predictors, train)
```
Partial regression plots show that they are linear.
